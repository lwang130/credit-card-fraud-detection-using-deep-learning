{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FIN580_Project1_RNN_Credit_Fraud.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gv-79uRaMWN",
        "colab_type": "text"
      },
      "source": [
        "# **Using RNN to Predict Credit Card Fraud**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwt07cUTbVRr",
        "colab_type": "text"
      },
      "source": [
        "*Lingyu He, Sixin Ma, Linchen Wang, Junyao Wang*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0r7Jc5Rmbuz",
        "colab_type": "text"
      },
      "source": [
        "**1. Import libraries and dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1fbAJpisEFZ",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "7b888694-c3e8-4934-d632-85e14b5f000b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf \n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlLV4BXd2jI0",
        "colab_type": "code",
        "outputId": "43ce7cf0-5f51-43cf-d642-81fcf49f05f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "2.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "259ebf3c-5c37-4f31-d8d8-79b07ed1c61d",
        "id": "rKhvCG08rZqM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN92lEKt9m1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuDNzm8B7Q1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#upload creditcard.csv to google drive -> Colab Notebooks before running this cell\n",
        "df = pd.read_csv(\"gdrive/My Drive/Colab Notebooks/creditcard.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhAvzq_wmpan",
        "colab_type": "text"
      },
      "source": [
        "**2.  Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7KzjA9k74c0",
        "colab_type": "code",
        "outputId": "01b307aa-9e46-41ba-da97-9585316efd94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltzbh3Cb9Ca8",
        "colab_type": "code",
        "outputId": "f76f300c-db3c-4ee8-dfa4-a8f6da48bbac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>284807.000000</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>284807.000000</td>\n",
              "      <td>284807.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>94813.859575</td>\n",
              "      <td>3.919560e-15</td>\n",
              "      <td>5.688174e-16</td>\n",
              "      <td>-8.769071e-15</td>\n",
              "      <td>2.782312e-15</td>\n",
              "      <td>-1.552563e-15</td>\n",
              "      <td>2.010663e-15</td>\n",
              "      <td>-1.694249e-15</td>\n",
              "      <td>-1.927028e-16</td>\n",
              "      <td>-3.137024e-15</td>\n",
              "      <td>1.768627e-15</td>\n",
              "      <td>9.170318e-16</td>\n",
              "      <td>-1.810658e-15</td>\n",
              "      <td>1.693438e-15</td>\n",
              "      <td>1.479045e-15</td>\n",
              "      <td>3.482336e-15</td>\n",
              "      <td>1.392007e-15</td>\n",
              "      <td>-7.528491e-16</td>\n",
              "      <td>4.328772e-16</td>\n",
              "      <td>9.049732e-16</td>\n",
              "      <td>5.085503e-16</td>\n",
              "      <td>1.537294e-16</td>\n",
              "      <td>7.959909e-16</td>\n",
              "      <td>5.367590e-16</td>\n",
              "      <td>4.458112e-15</td>\n",
              "      <td>1.453003e-15</td>\n",
              "      <td>1.699104e-15</td>\n",
              "      <td>-3.660161e-16</td>\n",
              "      <td>-1.206049e-16</td>\n",
              "      <td>88.349619</td>\n",
              "      <td>0.001727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>47488.145955</td>\n",
              "      <td>1.958696e+00</td>\n",
              "      <td>1.651309e+00</td>\n",
              "      <td>1.516255e+00</td>\n",
              "      <td>1.415869e+00</td>\n",
              "      <td>1.380247e+00</td>\n",
              "      <td>1.332271e+00</td>\n",
              "      <td>1.237094e+00</td>\n",
              "      <td>1.194353e+00</td>\n",
              "      <td>1.098632e+00</td>\n",
              "      <td>1.088850e+00</td>\n",
              "      <td>1.020713e+00</td>\n",
              "      <td>9.992014e-01</td>\n",
              "      <td>9.952742e-01</td>\n",
              "      <td>9.585956e-01</td>\n",
              "      <td>9.153160e-01</td>\n",
              "      <td>8.762529e-01</td>\n",
              "      <td>8.493371e-01</td>\n",
              "      <td>8.381762e-01</td>\n",
              "      <td>8.140405e-01</td>\n",
              "      <td>7.709250e-01</td>\n",
              "      <td>7.345240e-01</td>\n",
              "      <td>7.257016e-01</td>\n",
              "      <td>6.244603e-01</td>\n",
              "      <td>6.056471e-01</td>\n",
              "      <td>5.212781e-01</td>\n",
              "      <td>4.822270e-01</td>\n",
              "      <td>4.036325e-01</td>\n",
              "      <td>3.300833e-01</td>\n",
              "      <td>250.120109</td>\n",
              "      <td>0.041527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.640751e+01</td>\n",
              "      <td>-7.271573e+01</td>\n",
              "      <td>-4.832559e+01</td>\n",
              "      <td>-5.683171e+00</td>\n",
              "      <td>-1.137433e+02</td>\n",
              "      <td>-2.616051e+01</td>\n",
              "      <td>-4.355724e+01</td>\n",
              "      <td>-7.321672e+01</td>\n",
              "      <td>-1.343407e+01</td>\n",
              "      <td>-2.458826e+01</td>\n",
              "      <td>-4.797473e+00</td>\n",
              "      <td>-1.868371e+01</td>\n",
              "      <td>-5.791881e+00</td>\n",
              "      <td>-1.921433e+01</td>\n",
              "      <td>-4.498945e+00</td>\n",
              "      <td>-1.412985e+01</td>\n",
              "      <td>-2.516280e+01</td>\n",
              "      <td>-9.498746e+00</td>\n",
              "      <td>-7.213527e+00</td>\n",
              "      <td>-5.449772e+01</td>\n",
              "      <td>-3.483038e+01</td>\n",
              "      <td>-1.093314e+01</td>\n",
              "      <td>-4.480774e+01</td>\n",
              "      <td>-2.836627e+00</td>\n",
              "      <td>-1.029540e+01</td>\n",
              "      <td>-2.604551e+00</td>\n",
              "      <td>-2.256568e+01</td>\n",
              "      <td>-1.543008e+01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>54201.500000</td>\n",
              "      <td>-9.203734e-01</td>\n",
              "      <td>-5.985499e-01</td>\n",
              "      <td>-8.903648e-01</td>\n",
              "      <td>-8.486401e-01</td>\n",
              "      <td>-6.915971e-01</td>\n",
              "      <td>-7.682956e-01</td>\n",
              "      <td>-5.540759e-01</td>\n",
              "      <td>-2.086297e-01</td>\n",
              "      <td>-6.430976e-01</td>\n",
              "      <td>-5.354257e-01</td>\n",
              "      <td>-7.624942e-01</td>\n",
              "      <td>-4.055715e-01</td>\n",
              "      <td>-6.485393e-01</td>\n",
              "      <td>-4.255740e-01</td>\n",
              "      <td>-5.828843e-01</td>\n",
              "      <td>-4.680368e-01</td>\n",
              "      <td>-4.837483e-01</td>\n",
              "      <td>-4.988498e-01</td>\n",
              "      <td>-4.562989e-01</td>\n",
              "      <td>-2.117214e-01</td>\n",
              "      <td>-2.283949e-01</td>\n",
              "      <td>-5.423504e-01</td>\n",
              "      <td>-1.618463e-01</td>\n",
              "      <td>-3.545861e-01</td>\n",
              "      <td>-3.171451e-01</td>\n",
              "      <td>-3.269839e-01</td>\n",
              "      <td>-7.083953e-02</td>\n",
              "      <td>-5.295979e-02</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>84692.000000</td>\n",
              "      <td>1.810880e-02</td>\n",
              "      <td>6.548556e-02</td>\n",
              "      <td>1.798463e-01</td>\n",
              "      <td>-1.984653e-02</td>\n",
              "      <td>-5.433583e-02</td>\n",
              "      <td>-2.741871e-01</td>\n",
              "      <td>4.010308e-02</td>\n",
              "      <td>2.235804e-02</td>\n",
              "      <td>-5.142873e-02</td>\n",
              "      <td>-9.291738e-02</td>\n",
              "      <td>-3.275735e-02</td>\n",
              "      <td>1.400326e-01</td>\n",
              "      <td>-1.356806e-02</td>\n",
              "      <td>5.060132e-02</td>\n",
              "      <td>4.807155e-02</td>\n",
              "      <td>6.641332e-02</td>\n",
              "      <td>-6.567575e-02</td>\n",
              "      <td>-3.636312e-03</td>\n",
              "      <td>3.734823e-03</td>\n",
              "      <td>-6.248109e-02</td>\n",
              "      <td>-2.945017e-02</td>\n",
              "      <td>6.781943e-03</td>\n",
              "      <td>-1.119293e-02</td>\n",
              "      <td>4.097606e-02</td>\n",
              "      <td>1.659350e-02</td>\n",
              "      <td>-5.213911e-02</td>\n",
              "      <td>1.342146e-03</td>\n",
              "      <td>1.124383e-02</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>139320.500000</td>\n",
              "      <td>1.315642e+00</td>\n",
              "      <td>8.037239e-01</td>\n",
              "      <td>1.027196e+00</td>\n",
              "      <td>7.433413e-01</td>\n",
              "      <td>6.119264e-01</td>\n",
              "      <td>3.985649e-01</td>\n",
              "      <td>5.704361e-01</td>\n",
              "      <td>3.273459e-01</td>\n",
              "      <td>5.971390e-01</td>\n",
              "      <td>4.539234e-01</td>\n",
              "      <td>7.395934e-01</td>\n",
              "      <td>6.182380e-01</td>\n",
              "      <td>6.625050e-01</td>\n",
              "      <td>4.931498e-01</td>\n",
              "      <td>6.488208e-01</td>\n",
              "      <td>5.232963e-01</td>\n",
              "      <td>3.996750e-01</td>\n",
              "      <td>5.008067e-01</td>\n",
              "      <td>4.589494e-01</td>\n",
              "      <td>1.330408e-01</td>\n",
              "      <td>1.863772e-01</td>\n",
              "      <td>5.285536e-01</td>\n",
              "      <td>1.476421e-01</td>\n",
              "      <td>4.395266e-01</td>\n",
              "      <td>3.507156e-01</td>\n",
              "      <td>2.409522e-01</td>\n",
              "      <td>9.104512e-02</td>\n",
              "      <td>7.827995e-02</td>\n",
              "      <td>77.165000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>172792.000000</td>\n",
              "      <td>2.454930e+00</td>\n",
              "      <td>2.205773e+01</td>\n",
              "      <td>9.382558e+00</td>\n",
              "      <td>1.687534e+01</td>\n",
              "      <td>3.480167e+01</td>\n",
              "      <td>7.330163e+01</td>\n",
              "      <td>1.205895e+02</td>\n",
              "      <td>2.000721e+01</td>\n",
              "      <td>1.559499e+01</td>\n",
              "      <td>2.374514e+01</td>\n",
              "      <td>1.201891e+01</td>\n",
              "      <td>7.848392e+00</td>\n",
              "      <td>7.126883e+00</td>\n",
              "      <td>1.052677e+01</td>\n",
              "      <td>8.877742e+00</td>\n",
              "      <td>1.731511e+01</td>\n",
              "      <td>9.253526e+00</td>\n",
              "      <td>5.041069e+00</td>\n",
              "      <td>5.591971e+00</td>\n",
              "      <td>3.942090e+01</td>\n",
              "      <td>2.720284e+01</td>\n",
              "      <td>1.050309e+01</td>\n",
              "      <td>2.252841e+01</td>\n",
              "      <td>4.584549e+00</td>\n",
              "      <td>7.519589e+00</td>\n",
              "      <td>3.517346e+00</td>\n",
              "      <td>3.161220e+01</td>\n",
              "      <td>3.384781e+01</td>\n",
              "      <td>25691.160000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Time            V1  ...         Amount          Class\n",
              "count  284807.000000  2.848070e+05  ...  284807.000000  284807.000000\n",
              "mean    94813.859575  3.919560e-15  ...      88.349619       0.001727\n",
              "std     47488.145955  1.958696e+00  ...     250.120109       0.041527\n",
              "min         0.000000 -5.640751e+01  ...       0.000000       0.000000\n",
              "25%     54201.500000 -9.203734e-01  ...       5.600000       0.000000\n",
              "50%     84692.000000  1.810880e-02  ...      22.000000       0.000000\n",
              "75%    139320.500000  1.315642e+00  ...      77.165000       0.000000\n",
              "max    172792.000000  2.454930e+00  ...   25691.160000       1.000000\n",
              "\n",
              "[8 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXFgqwgZ9ESL",
        "colab_type": "code",
        "outputId": "33a95af2-9751-49d5-a98c-f48b47ef8b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "correlation = df.corr()\n",
        "sns.heatmap(correlation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efc58adc7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAESCAYAAAD0aQL3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJKkmSWxm9Yc",
        "colab_type": "text"
      },
      "source": [
        "**3. Prepare Data for ML Algo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXEFSSkdnFjQ",
        "colab_type": "text"
      },
      "source": [
        "3.1 Train-test-split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKXMYruw9s8r",
        "colab_type": "code",
        "outputId": "4085c485-0a6e-4129-d834-4f7b6733046b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "features = df.drop('Class', axis = 1)\n",
        "target = df['Class']\n",
        "\n",
        "print(features.shape)\n",
        "print(target.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(284807, 30)\n",
            "(284807,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Du3mMZ-KU4",
        "colab_type": "code",
        "outputId": "b189b1d8-163c-47dc-9dc3-b8c277bc7b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "features.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V26       V27       V28  Amount\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ... -0.189115  0.133558 -0.021053  149.62\n",
              "1   0.0  1.191857  0.266151  0.166480  ...  0.125895 -0.008983  0.014724    2.69\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.139097 -0.055353 -0.059752  378.66\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ... -0.221929  0.062723  0.061458  123.50\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.502292  0.219422  0.215153   69.99\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdycvPZk-hcU",
        "colab_type": "code",
        "outputId": "fdd270e1-9e75-4114-cab8-b97c619e18fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "target.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMOV4ZOw-jcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2flrlmc-xor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, target,test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJnp1T4FmTCX",
        "colab_type": "text"
      },
      "source": [
        "3.2 Standarize and tranform the data for RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4W_380M-0Cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_trans = scaler.transform(X_train)\n",
        "X_test_trans = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI0_wWLf-8mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_train_trans)\n",
        "X_train_pca = pca.transform(X_train_trans)\n",
        "X_test_pca = pca.transform(X_test_trans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg3uh7Pjnnkw",
        "colab_type": "text"
      },
      "source": [
        "**4. Benchmark: Logistic Regression and RandomForest Classifier(roughly tuned)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_4lZa1Lns6-",
        "colab_type": "text"
      },
      "source": [
        "4.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R07Dr7c9_ObI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtSg5L92_UcS",
        "colab_type": "code",
        "outputId": "57db7bc3-6991-4844-9dfb-a9d985a0a5eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_pca, y_train)\n",
        "scores = cross_val_score(lr, X_train_pca, y_train, cv = 10, scoring = 'roc_auc')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jybbzSeU_hib",
        "colab_type": "code",
        "outputId": "d0d2b904-bde7-4100-b138-5478815127f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(scores)\n",
        "print(scores.mean(), scores.std())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.98915743 0.97604858 0.95539679 0.98229721 0.97075491 0.96889934\n",
            " 0.99619753 0.96266297 0.9709319  0.99887944]\n",
            "0.9771226105326123 0.013555686018750741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlR6j9iAAHHs",
        "colab_type": "code",
        "outputId": "ccbc0a2b-f3f5-4340-faa4-2f048d7bcafe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = lr.predict(X_test_pca)\n",
        "conf_matrix = confusion_matrix(y_test, y_predict)\n",
        "print(conf_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[56855     9]\n",
            " [   41    57]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N44G_Zm5AMNK",
        "colab_type": "code",
        "outputId": "4de6efeb-50c9-448f-a007-327750e34539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fraud_conditional_prob = conf_matrix[1,1] / (conf_matrix[1,0] + conf_matrix[1,1])\n",
        "print(fraud_conditional_prob)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5816326530612245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5___Ybhny-q",
        "colab_type": "text"
      },
      "source": [
        "4.2 RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGfDKA7OC49p",
        "colab_type": "code",
        "outputId": "6f43adcd-8a26-4ff3-f8f3-5ebd560d1858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
        "train_results = []\n",
        "test_results = []\n",
        "for max_depth in max_depths:\n",
        "   rf = RandomForestClassifier(max_depth=max_depth, n_jobs=-1)\n",
        "   rf.fit(X_train_pca, y_train)\n",
        "   train_pred = rf.predict(X_train_pca)\n",
        "   test_pred = rf.predict(X_test_pca)\n",
        "   train_results.append(metrics.accuracy_score(train_pred,y_train)) \n",
        "   test_results.append(metrics.accuracy_score(test_pred,y_test)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPdFIFc3DCCy",
        "colab_type": "code",
        "outputId": "b2b3433f-bfb5-4065-c60d-1f165b81f131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "train_results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9983760890078781,\n",
              " 0.9987140380521846,\n",
              " 0.9990914876341372,\n",
              " 0.9993372687572692,\n",
              " 0.9994074919353069,\n",
              " 0.9995347714455002,\n",
              " 0.9995918277776559,\n",
              " 0.9996444951611841,\n",
              " 0.9996620509556936,\n",
              " 0.9996927735960851,\n",
              " 0.9997147183392219,\n",
              " 0.9997191072878492,\n",
              " 0.9997366630823586,\n",
              " 0.9997322741337312,\n",
              " 0.9997498299282407,\n",
              " 0.9997629967741227,\n",
              " 0.9998068862603964,\n",
              " 0.999815664157651,\n",
              " 0.9998376089007878,\n",
              " 0.9998814983870614,\n",
              " 0.9998946652329435,\n",
              " 0.999877109438434,\n",
              " 0.9998946652329435,\n",
              " 0.9998946652329435,\n",
              " 0.999890276284316,\n",
              " 0.9999209989247075,\n",
              " 0.9999166099760802,\n",
              " 0.9998990541815708,\n",
              " 0.9999034431301982,\n",
              " 0.9998858873356887,\n",
              " 0.9999122210274529,\n",
              " 0.9998990541815708]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvLXYDIGDClr",
        "colab_type": "code",
        "outputId": "f01d4f55-669f-4ba9-c977-b77e8c480722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "test_results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9983673326077034,\n",
              " 0.9986306660580738,\n",
              " 0.9990344440153085,\n",
              " 0.9991222218320986,\n",
              " 0.9992802219023208,\n",
              " 0.999403110845827,\n",
              " 0.9994382219725431,\n",
              " 0.9994733330992591,\n",
              " 0.9994908886626171,\n",
              " 0.9995084442259752,\n",
              " 0.9994733330992591,\n",
              " 0.9994557775359011,\n",
              " 0.9994908886626171,\n",
              " 0.9994908886626171,\n",
              " 0.999420666409185,\n",
              " 0.9995435553526912,\n",
              " 0.9994733330992591,\n",
              " 0.9994382219725431,\n",
              " 0.999385555282469,\n",
              " 0.9994382219725431,\n",
              " 0.9994557775359011,\n",
              " 0.9994908886626171,\n",
              " 0.9994557775359011,\n",
              " 0.9995435553526912,\n",
              " 0.9994733330992591,\n",
              " 0.9993679997191109,\n",
              " 0.9994908886626171,\n",
              " 0.999420666409185,\n",
              " 0.999403110845827,\n",
              " 0.9994382219725431,\n",
              " 0.9994382219725431,\n",
              " 0.9994382219725431]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQgRgyNNAqsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators = 10, max_depth=16, n_jobs=-1)\n",
        "rf.fit(X_train_pca, y_train)\n",
        "train_pred = rf.predict(X_train_pca)\n",
        "test_pred = rf.predict(X_test_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpcMvc7HBHEQ",
        "colab_type": "code",
        "outputId": "b6cbf837-900b-44b9-92b1-b6fffd4336fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "conf_matrix = confusion_matrix(y_test, test_pred)\n",
        "print(conf_matrix)\n",
        "\n",
        "fraud_conditional_prob = conf_matrix[1,1] / (conf_matrix[1,0] + conf_matrix[1,1])\n",
        "print(fraud_conditional_prob)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[56863     1]\n",
            " [   28    70]]\n",
            "0.7142857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVBXaYq4n5Hu",
        "colab_type": "text"
      },
      "source": [
        "Random Forest Classfier gives predictions with high accuracies. However, it mispredicts 29 percent of class 1 (fraud), the performance is relatively low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlV3Sn2_ogPV",
        "colab_type": "text"
      },
      "source": [
        "**5. RNN Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns82tupFBbs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import embeddings\n",
        "from keras import optimizers\n",
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CTXl-ZTDdgv",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#Failed Model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(240, input_shape = (30,),activation = \"relu\")) #dense1\n",
        "\n",
        "model.add(Dense(120, activation = \"relu\")) #dense 2\n",
        "\n",
        "model.add(Dense(120, activation = \"relu\")) #dense 2\n",
        "\n",
        "model.add(Dense(60, activation = \"relu\")) #dense 2\n",
        "\n",
        "model.add(Dense(1, activation = None)) #logit\n",
        "\n",
        "model.add(Dense(1, activation  = \"relu\")) #output (using softmax as the activation function yields a low accuracy= 0.016) \n",
        "\n",
        "opt = optimizers.Adam(lr = 0.0001, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
        "model.compile(loss='binary_crossentropy', optimizer = opt,  metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjUj0sqqoxf5",
        "colab_type": "text"
      },
      "source": [
        "Use binary_crossentropy as loss function, output layer using sigmoid as activation function, because of the nature of binary classification problem\n",
        "\n",
        "All layers are dense. Gradually increase the number of hidden layer and the number of units inside a layer until getting a reasonable result.\n",
        "\n",
        "Decrease the learning rate until it converges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heX-AIAcAD_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Working Model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(480, input_shape = (30,),activation = \"relu\")) #dense1\n",
        "\n",
        "model.add(Dense(480, activation = \"relu\")) \n",
        "\n",
        "model.add(Dense(240,activation = \"relu\"))\n",
        "\n",
        "model.add(Dense(120, activation = \"relu\")) #dense 2\n",
        "\n",
        "model.add(Dense(120, activation = \"relu\")) #dense 2\n",
        "\n",
        "model.add(Dense(60, activation = \"relu\")) #dense 2\n",
        "\n",
        "model.add(Dense(60, activation = \"relu\"))\n",
        "\n",
        "model.add(Dense(1, activation = \"relu\")) #logit\n",
        "\n",
        "model.add(Dense(1, activation  = \"sigmoid\")) #output (using softmax as the activation function yields a low accuracy= 0.016) \n",
        "\n",
        "opt = optimizers.Adam(lr = 0.0008, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
        "model.compile(loss='binary_crossentropy', optimizer = opt, metrics = ['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afs5384Y-uw3",
        "colab_type": "code",
        "outputId": "77b7d5e5-8726-43f7-ad2a-e285b12c7642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history_2 = model.fit(X_train_pca, y_train, epochs = 1000, batch_size = 5000, class_weight={0 : 1., 1: (1/np.mean(y_train))}, validation_data=(X_test_pca, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 227845 samples, validate on 56962 samples\n",
            "Epoch 1/1000\n",
            "227845/227845 [==============================] - 2s 8us/step - loss: 0.8709 - acc: 0.8073 - val_loss: 0.6820 - val_acc: 0.9990\n",
            "Epoch 2/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.8027 - acc: 0.9963 - val_loss: 0.6629 - val_acc: 0.9993\n",
            "Epoch 3/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.7702 - acc: 0.9905 - val_loss: 0.6486 - val_acc: 0.9956\n",
            "Epoch 4/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.7460 - acc: 0.9815 - val_loss: 0.6360 - val_acc: 0.9865\n",
            "Epoch 5/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.7197 - acc: 0.9907 - val_loss: 0.6158 - val_acc: 0.9943\n",
            "Epoch 6/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.6996 - acc: 0.9947 - val_loss: 0.5987 - val_acc: 0.9978\n",
            "Epoch 7/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.6819 - acc: 0.9967 - val_loss: 0.5835 - val_acc: 0.9978\n",
            "Epoch 8/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.6769 - acc: 0.9946 - val_loss: 0.5754 - val_acc: 0.9921\n",
            "Epoch 9/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.6630 - acc: 0.9948 - val_loss: 0.5565 - val_acc: 0.9955\n",
            "Epoch 10/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.6426 - acc: 0.9966 - val_loss: 0.5440 - val_acc: 0.9959\n",
            "Epoch 11/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.6301 - acc: 0.9981 - val_loss: 0.5248 - val_acc: 0.9991\n",
            "Epoch 12/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.6206 - acc: 0.9974 - val_loss: 0.5145 - val_acc: 0.9979\n",
            "Epoch 13/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.6091 - acc: 0.9973 - val_loss: 0.5026 - val_acc: 0.9976\n",
            "Epoch 14/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.5955 - acc: 0.9984 - val_loss: 0.4909 - val_acc: 0.9978\n",
            "Epoch 15/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.6022 - acc: 0.9912 - val_loss: 0.4801 - val_acc: 0.9929\n",
            "Epoch 16/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.5864 - acc: 0.9931 - val_loss: 0.4974 - val_acc: 0.9799\n",
            "Epoch 17/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.5795 - acc: 0.9901 - val_loss: 0.5265 - val_acc: 0.9675\n",
            "Epoch 18/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.5850 - acc: 0.9732 - val_loss: 0.4604 - val_acc: 0.9840\n",
            "Epoch 19/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.5568 - acc: 0.9925 - val_loss: 0.4355 - val_acc: 0.9955\n",
            "Epoch 20/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.5441 - acc: 0.9973 - val_loss: 0.4254 - val_acc: 0.9972\n",
            "Epoch 21/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.5344 - acc: 0.9949 - val_loss: 0.4183 - val_acc: 0.9959\n",
            "Epoch 22/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.5234 - acc: 0.9946 - val_loss: 0.4040 - val_acc: 0.9967\n",
            "Epoch 23/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.5083 - acc: 0.9974 - val_loss: 0.3936 - val_acc: 0.9986\n",
            "Epoch 24/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.5108 - acc: 0.9984 - val_loss: 0.3848 - val_acc: 0.9987\n",
            "Epoch 25/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.5177 - acc: 0.9956 - val_loss: 0.3805 - val_acc: 0.9963\n",
            "Epoch 26/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4918 - acc: 0.9974 - val_loss: 0.3739 - val_acc: 0.9966\n",
            "Epoch 27/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4837 - acc: 0.9981 - val_loss: 0.3619 - val_acc: 0.9984\n",
            "Epoch 28/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4777 - acc: 0.9983 - val_loss: 0.3568 - val_acc: 0.9977\n",
            "Epoch 29/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4937 - acc: 0.9929 - val_loss: 0.4172 - val_acc: 0.9697\n",
            "Epoch 30/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4878 - acc: 0.9880 - val_loss: 0.3459 - val_acc: 0.9920\n",
            "Epoch 31/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4679 - acc: 0.9931 - val_loss: 0.3396 - val_acc: 0.9928\n",
            "Epoch 32/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4569 - acc: 0.9954 - val_loss: 0.3335 - val_acc: 0.9932\n",
            "Epoch 33/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4495 - acc: 0.9959 - val_loss: 0.3191 - val_acc: 0.9977\n",
            "Epoch 34/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4542 - acc: 0.9980 - val_loss: 0.3112 - val_acc: 0.9989\n",
            "Epoch 35/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4480 - acc: 0.9942 - val_loss: 0.3129 - val_acc: 0.9958\n",
            "Epoch 36/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4326 - acc: 0.9968 - val_loss: 0.3089 - val_acc: 0.9950\n",
            "Epoch 37/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4254 - acc: 0.9968 - val_loss: 0.2972 - val_acc: 0.9976\n",
            "Epoch 38/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4185 - acc: 0.9981 - val_loss: 0.2898 - val_acc: 0.9986\n",
            "Epoch 39/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4130 - acc: 0.9987 - val_loss: 0.2842 - val_acc: 0.9988\n",
            "Epoch 40/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4125 - acc: 0.9989 - val_loss: 0.2788 - val_acc: 0.9990\n",
            "Epoch 41/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4228 - acc: 0.9972 - val_loss: 0.6785 - val_acc: 0.9515\n",
            "Epoch 42/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4760 - acc: 0.9876 - val_loss: 0.2886 - val_acc: 0.9909\n",
            "Epoch 43/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4235 - acc: 0.9929 - val_loss: 0.2731 - val_acc: 0.9949\n",
            "Epoch 44/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4145 - acc: 0.9954 - val_loss: 0.2715 - val_acc: 0.9952\n",
            "Epoch 45/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4076 - acc: 0.9964 - val_loss: 0.2577 - val_acc: 0.9981\n",
            "Epoch 46/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4014 - acc: 0.9977 - val_loss: 0.2559 - val_acc: 0.9979\n",
            "Epoch 47/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4062 - acc: 0.9984 - val_loss: 0.2587 - val_acc: 0.9967\n",
            "Epoch 48/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3963 - acc: 0.9973 - val_loss: 0.2472 - val_acc: 0.9979\n",
            "Epoch 49/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4155 - acc: 0.9967 - val_loss: 0.4347 - val_acc: 0.9656\n",
            "Epoch 50/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4371 - acc: 0.9760 - val_loss: 0.2546 - val_acc: 0.9844\n",
            "Epoch 51/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4122 - acc: 0.9849 - val_loss: 0.2441 - val_acc: 0.9903\n",
            "Epoch 52/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4065 - acc: 0.9857 - val_loss: 0.2544 - val_acc: 0.9814\n",
            "Epoch 53/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4033 - acc: 0.9847 - val_loss: 0.2503 - val_acc: 0.9803\n",
            "Epoch 54/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3961 - acc: 0.9849 - val_loss: 0.2325 - val_acc: 0.9928\n",
            "Epoch 55/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3938 - acc: 0.9875 - val_loss: 0.2283 - val_acc: 0.9939\n",
            "Epoch 56/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3874 - acc: 0.9928 - val_loss: 0.2285 - val_acc: 0.9922\n",
            "Epoch 57/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3813 - acc: 0.9918 - val_loss: 0.2236 - val_acc: 0.9940\n",
            "Epoch 58/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3735 - acc: 0.9939 - val_loss: 0.2160 - val_acc: 0.9963\n",
            "Epoch 59/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3697 - acc: 0.9954 - val_loss: 0.2169 - val_acc: 0.9952\n",
            "Epoch 60/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3650 - acc: 0.9962 - val_loss: 0.2140 - val_acc: 0.9955\n",
            "Epoch 61/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3593 - acc: 0.9972 - val_loss: 0.2067 - val_acc: 0.9978\n",
            "Epoch 62/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3623 - acc: 0.9977 - val_loss: 0.2050 - val_acc: 0.9978\n",
            "Epoch 63/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3639 - acc: 0.9968 - val_loss: 0.2268 - val_acc: 0.9899\n",
            "Epoch 64/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3696 - acc: 0.9930 - val_loss: 0.2017 - val_acc: 0.9962\n",
            "Epoch 65/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3590 - acc: 0.9961 - val_loss: 0.1992 - val_acc: 0.9967\n",
            "Epoch 66/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3583 - acc: 0.9969 - val_loss: 0.1959 - val_acc: 0.9974\n",
            "Epoch 67/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3551 - acc: 0.9975 - val_loss: 0.1920 - val_acc: 0.9983\n",
            "Epoch 68/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3632 - acc: 0.9954 - val_loss: 0.2217 - val_acc: 0.9875\n",
            "Epoch 69/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3577 - acc: 0.9937 - val_loss: 0.1918 - val_acc: 0.9951\n",
            "Epoch 70/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3532 - acc: 0.9953 - val_loss: 0.1922 - val_acc: 0.9944\n",
            "Epoch 71/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3495 - acc: 0.9960 - val_loss: 0.1875 - val_acc: 0.9959\n",
            "Epoch 72/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3458 - acc: 0.9966 - val_loss: 0.1825 - val_acc: 0.9973\n",
            "Epoch 73/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3483 - acc: 0.9972 - val_loss: 0.1801 - val_acc: 0.9977\n",
            "Epoch 74/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3460 - acc: 0.9977 - val_loss: 0.1796 - val_acc: 0.9976\n",
            "Epoch 75/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3441 - acc: 0.9980 - val_loss: 0.1763 - val_acc: 0.9982\n",
            "Epoch 76/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3429 - acc: 0.9982 - val_loss: 0.1771 - val_acc: 0.9975\n",
            "Epoch 77/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3426 - acc: 0.9981 - val_loss: 0.1727 - val_acc: 0.9983\n",
            "Epoch 78/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3406 - acc: 0.9985 - val_loss: 0.1718 - val_acc: 0.9983\n",
            "Epoch 79/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3396 - acc: 0.9985 - val_loss: 0.1697 - val_acc: 0.9986\n",
            "Epoch 80/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3382 - acc: 0.9987 - val_loss: 0.1669 - val_acc: 0.9989\n",
            "Epoch 81/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3370 - acc: 0.9988 - val_loss: 0.1650 - val_acc: 0.9990\n",
            "Epoch 82/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3557 - acc: 0.9989 - val_loss: 0.1638 - val_acc: 0.9990\n",
            "Epoch 83/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3645 - acc: 0.9991 - val_loss: 0.1632 - val_acc: 0.9990\n",
            "Epoch 84/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.4052 - acc: 0.9836 - val_loss: 0.1879 - val_acc: 0.9866\n",
            "Epoch 85/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3542 - acc: 0.9917 - val_loss: 0.1748 - val_acc: 0.9913\n",
            "Epoch 86/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3480 - acc: 0.9933 - val_loss: 0.1711 - val_acc: 0.9929\n",
            "Epoch 87/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3435 - acc: 0.9942 - val_loss: 0.1654 - val_acc: 0.9949\n",
            "Epoch 88/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3389 - acc: 0.9956 - val_loss: 0.1635 - val_acc: 0.9955\n",
            "Epoch 89/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3369 - acc: 0.9958 - val_loss: 0.1624 - val_acc: 0.9959\n",
            "Epoch 90/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3430 - acc: 0.9962 - val_loss: 0.1860 - val_acc: 0.9917\n",
            "Epoch 91/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3393 - acc: 0.9968 - val_loss: 0.1656 - val_acc: 0.9955\n",
            "Epoch 92/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3358 - acc: 0.9962 - val_loss: 0.1586 - val_acc: 0.9969\n",
            "Epoch 93/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3314 - acc: 0.9974 - val_loss: 0.1580 - val_acc: 0.9969\n",
            "Epoch 94/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3293 - acc: 0.9977 - val_loss: 0.1529 - val_acc: 0.9981\n",
            "Epoch 95/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3282 - acc: 0.9980 - val_loss: 0.1524 - val_acc: 0.9981\n",
            "Epoch 96/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3274 - acc: 0.9981 - val_loss: 0.1510 - val_acc: 0.9983\n",
            "Epoch 97/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3260 - acc: 0.9984 - val_loss: 0.1496 - val_acc: 0.9984\n",
            "Epoch 98/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3251 - acc: 0.9984 - val_loss: 0.1482 - val_acc: 0.9985\n",
            "Epoch 99/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3245 - acc: 0.9985 - val_loss: 0.1471 - val_acc: 0.9986\n",
            "Epoch 100/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3238 - acc: 0.9985 - val_loss: 0.1453 - val_acc: 0.9987\n",
            "Epoch 101/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3231 - acc: 0.9988 - val_loss: 0.1459 - val_acc: 0.9985\n",
            "Epoch 102/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3233 - acc: 0.9984 - val_loss: 0.1435 - val_acc: 0.9988\n",
            "Epoch 103/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3215 - acc: 0.9989 - val_loss: 0.1432 - val_acc: 0.9987\n",
            "Epoch 104/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3247 - acc: 0.9983 - val_loss: 0.1464 - val_acc: 0.9981\n",
            "Epoch 105/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3226 - acc: 0.9987 - val_loss: 0.1423 - val_acc: 0.9985\n",
            "Epoch 106/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3214 - acc: 0.9986 - val_loss: 0.1409 - val_acc: 0.9986\n",
            "Epoch 107/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3518 - acc: 0.9942 - val_loss: 0.1485 - val_acc: 0.9969\n",
            "Epoch 108/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3612 - acc: 0.9981 - val_loss: 0.1421 - val_acc: 0.9980\n",
            "Epoch 109/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3391 - acc: 0.9954 - val_loss: 0.1439 - val_acc: 0.9962\n",
            "Epoch 110/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3243 - acc: 0.9973 - val_loss: 0.1399 - val_acc: 0.9973\n",
            "Epoch 111/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3230 - acc: 0.9973 - val_loss: 0.1387 - val_acc: 0.9976\n",
            "Epoch 112/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3216 - acc: 0.9977 - val_loss: 0.1375 - val_acc: 0.9981\n",
            "Epoch 113/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3211 - acc: 0.9980 - val_loss: 0.1366 - val_acc: 0.9982\n",
            "Epoch 114/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3199 - acc: 0.9982 - val_loss: 0.1357 - val_acc: 0.9983\n",
            "Epoch 115/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3192 - acc: 0.9984 - val_loss: 0.1342 - val_acc: 0.9987\n",
            "Epoch 116/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3188 - acc: 0.9985 - val_loss: 0.1340 - val_acc: 0.9986\n",
            "Epoch 117/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3182 - acc: 0.9987 - val_loss: 0.1329 - val_acc: 0.9987\n",
            "Epoch 118/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3175 - acc: 0.9987 - val_loss: 0.1318 - val_acc: 0.9989\n",
            "Epoch 119/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3172 - acc: 0.9989 - val_loss: 0.1318 - val_acc: 0.9988\n",
            "Epoch 120/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3166 - acc: 0.9989 - val_loss: 0.1304 - val_acc: 0.9990\n",
            "Epoch 121/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3164 - acc: 0.9990 - val_loss: 0.1299 - val_acc: 0.9990\n",
            "Epoch 122/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3158 - acc: 0.9991 - val_loss: 0.1297 - val_acc: 0.9990\n",
            "Epoch 123/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3159 - acc: 0.9990 - val_loss: 0.1283 - val_acc: 0.9990\n",
            "Epoch 124/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3261 - acc: 0.9992 - val_loss: 0.1279 - val_acc: 0.9991\n",
            "Epoch 125/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3257 - acc: 0.9992 - val_loss: 0.1286 - val_acc: 0.9989\n",
            "Epoch 126/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3889 - acc: 0.9912 - val_loss: 0.4542 - val_acc: 0.9566\n",
            "Epoch 127/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3822 - acc: 0.9826 - val_loss: 0.1501 - val_acc: 0.9877\n",
            "Epoch 128/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3476 - acc: 0.9883 - val_loss: 0.1504 - val_acc: 0.9874\n",
            "Epoch 129/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3429 - acc: 0.9904 - val_loss: 0.1526 - val_acc: 0.9865\n",
            "Epoch 130/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3405 - acc: 0.9897 - val_loss: 0.1471 - val_acc: 0.9887\n",
            "Epoch 131/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3373 - acc: 0.9909 - val_loss: 0.1425 - val_acc: 0.9907\n",
            "Epoch 132/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3342 - acc: 0.9916 - val_loss: 0.1450 - val_acc: 0.9894\n",
            "Epoch 133/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3312 - acc: 0.9924 - val_loss: 0.1389 - val_acc: 0.9919\n",
            "Epoch 134/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3299 - acc: 0.9918 - val_loss: 0.1334 - val_acc: 0.9939\n",
            "Epoch 135/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3263 - acc: 0.9927 - val_loss: 0.1350 - val_acc: 0.9930\n",
            "Epoch 136/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3243 - acc: 0.9929 - val_loss: 0.1304 - val_acc: 0.9945\n",
            "Epoch 137/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3222 - acc: 0.9936 - val_loss: 0.1304 - val_acc: 0.9942\n",
            "Epoch 138/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3196 - acc: 0.9946 - val_loss: 0.1311 - val_acc: 0.9937\n",
            "Epoch 139/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3176 - acc: 0.9948 - val_loss: 0.1290 - val_acc: 0.9943\n",
            "Epoch 140/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3159 - acc: 0.9951 - val_loss: 0.1255 - val_acc: 0.9959\n",
            "Epoch 141/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3149 - acc: 0.9953 - val_loss: 0.1251 - val_acc: 0.9958\n",
            "Epoch 142/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3131 - acc: 0.9957 - val_loss: 0.1242 - val_acc: 0.9962\n",
            "Epoch 143/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3117 - acc: 0.9964 - val_loss: 0.1231 - val_acc: 0.9967\n",
            "Epoch 144/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3105 - acc: 0.9966 - val_loss: 0.1211 - val_acc: 0.9973\n",
            "Epoch 145/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3093 - acc: 0.9969 - val_loss: 0.1217 - val_acc: 0.9969\n",
            "Epoch 146/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3082 - acc: 0.9971 - val_loss: 0.1227 - val_acc: 0.9964\n",
            "Epoch 147/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3070 - acc: 0.9972 - val_loss: 0.1222 - val_acc: 0.9964\n",
            "Epoch 148/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3057 - acc: 0.9971 - val_loss: 0.1189 - val_acc: 0.9976\n",
            "Epoch 149/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3045 - acc: 0.9978 - val_loss: 0.1173 - val_acc: 0.9983\n",
            "Epoch 150/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3034 - acc: 0.9982 - val_loss: 0.1191 - val_acc: 0.9975\n",
            "Epoch 151/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3028 - acc: 0.9982 - val_loss: 0.1163 - val_acc: 0.9984\n",
            "Epoch 152/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3023 - acc: 0.9984 - val_loss: 0.1171 - val_acc: 0.9982\n",
            "Epoch 153/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3019 - acc: 0.9985 - val_loss: 0.1158 - val_acc: 0.9985\n",
            "Epoch 154/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3013 - acc: 0.9986 - val_loss: 0.1151 - val_acc: 0.9986\n",
            "Epoch 155/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3012 - acc: 0.9986 - val_loss: 0.1143 - val_acc: 0.9987\n",
            "Epoch 156/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3007 - acc: 0.9988 - val_loss: 0.1136 - val_acc: 0.9987\n",
            "Epoch 157/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3008 - acc: 0.9987 - val_loss: 0.1147 - val_acc: 0.9985\n",
            "Epoch 158/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3003 - acc: 0.9987 - val_loss: 0.1131 - val_acc: 0.9987\n",
            "Epoch 159/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2998 - acc: 0.9988 - val_loss: 0.1127 - val_acc: 0.9987\n",
            "Epoch 160/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2994 - acc: 0.9989 - val_loss: 0.1117 - val_acc: 0.9988\n",
            "Epoch 161/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2992 - acc: 0.9989 - val_loss: 0.1115 - val_acc: 0.9988\n",
            "Epoch 162/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2989 - acc: 0.9990 - val_loss: 0.1105 - val_acc: 0.9989\n",
            "Epoch 163/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3024 - acc: 0.9984 - val_loss: 0.1115 - val_acc: 0.9987\n",
            "Epoch 164/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2998 - acc: 0.9989 - val_loss: 0.1110 - val_acc: 0.9987\n",
            "Epoch 165/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2987 - acc: 0.9990 - val_loss: 0.1102 - val_acc: 0.9988\n",
            "Epoch 166/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2984 - acc: 0.9990 - val_loss: 0.1099 - val_acc: 0.9988\n",
            "Epoch 167/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2981 - acc: 0.9990 - val_loss: 0.1090 - val_acc: 0.9990\n",
            "Epoch 168/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2979 - acc: 0.9990 - val_loss: 0.1093 - val_acc: 0.9988\n",
            "Epoch 169/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2977 - acc: 0.9991 - val_loss: 0.1085 - val_acc: 0.9990\n",
            "Epoch 170/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2975 - acc: 0.9991 - val_loss: 0.1083 - val_acc: 0.9989\n",
            "Epoch 171/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2972 - acc: 0.9992 - val_loss: 0.1075 - val_acc: 0.9991\n",
            "Epoch 172/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2972 - acc: 0.9992 - val_loss: 0.1075 - val_acc: 0.9990\n",
            "Epoch 173/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2972 - acc: 0.9992 - val_loss: 0.1098 - val_acc: 0.9987\n",
            "Epoch 174/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2981 - acc: 0.9990 - val_loss: 0.1071 - val_acc: 0.9990\n",
            "Epoch 175/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2984 - acc: 0.9990 - val_loss: 0.1104 - val_acc: 0.9985\n",
            "Epoch 176/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2980 - acc: 0.9990 - val_loss: 0.1061 - val_acc: 0.9990\n",
            "Epoch 177/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2971 - acc: 0.9991 - val_loss: 0.1063 - val_acc: 0.9989\n",
            "Epoch 178/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2967 - acc: 0.9992 - val_loss: 0.1056 - val_acc: 0.9990\n",
            "Epoch 179/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2966 - acc: 0.9992 - val_loss: 0.1052 - val_acc: 0.9991\n",
            "Epoch 180/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2964 - acc: 0.9993 - val_loss: 0.1051 - val_acc: 0.9991\n",
            "Epoch 181/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2963 - acc: 0.9992 - val_loss: 0.1049 - val_acc: 0.9991\n",
            "Epoch 182/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2961 - acc: 0.9993 - val_loss: 0.1044 - val_acc: 0.9991\n",
            "Epoch 183/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2960 - acc: 0.9993 - val_loss: 0.1044 - val_acc: 0.9991\n",
            "Epoch 184/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2959 - acc: 0.9994 - val_loss: 0.1043 - val_acc: 0.9991\n",
            "Epoch 185/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2958 - acc: 0.9994 - val_loss: 0.1039 - val_acc: 0.9992\n",
            "Epoch 186/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2962 - acc: 0.9993 - val_loss: 0.1047 - val_acc: 0.9990\n",
            "Epoch 187/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2959 - acc: 0.9994 - val_loss: 0.1033 - val_acc: 0.9992\n",
            "Epoch 188/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2957 - acc: 0.9994 - val_loss: 0.1033 - val_acc: 0.9991\n",
            "Epoch 189/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2955 - acc: 0.9994 - val_loss: 0.1028 - val_acc: 0.9992\n",
            "Epoch 190/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2954 - acc: 0.9994 - val_loss: 0.1026 - val_acc: 0.9992\n",
            "Epoch 191/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2971 - acc: 0.9991 - val_loss: 0.1027 - val_acc: 0.9991\n",
            "Epoch 192/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2959 - acc: 0.9993 - val_loss: 0.1021 - val_acc: 0.9991\n",
            "Epoch 193/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2962 - acc: 0.9992 - val_loss: 0.1018 - val_acc: 0.9991\n",
            "Epoch 194/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2954 - acc: 0.9994 - val_loss: 0.1018 - val_acc: 0.9992\n",
            "Epoch 195/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2953 - acc: 0.9994 - val_loss: 0.1018 - val_acc: 0.9991\n",
            "Epoch 196/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2952 - acc: 0.9994 - val_loss: 0.1016 - val_acc: 0.9991\n",
            "Epoch 197/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2951 - acc: 0.9994 - val_loss: 0.1010 - val_acc: 0.9992\n",
            "Epoch 198/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2951 - acc: 0.9994 - val_loss: 0.1011 - val_acc: 0.9991\n",
            "Epoch 199/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2956 - acc: 0.9993 - val_loss: 0.1008 - val_acc: 0.9991\n",
            "Epoch 200/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2953 - acc: 0.9994 - val_loss: 0.1005 - val_acc: 0.9992\n",
            "Epoch 201/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3001 - acc: 0.9987 - val_loss: 0.1053 - val_acc: 0.9984\n",
            "Epoch 202/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2983 - acc: 0.9989 - val_loss: 0.1012 - val_acc: 0.9989\n",
            "Epoch 203/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2964 - acc: 0.9991 - val_loss: 0.1005 - val_acc: 0.9990\n",
            "Epoch 204/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2955 - acc: 0.9992 - val_loss: 0.1000 - val_acc: 0.9991\n",
            "Epoch 205/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2953 - acc: 0.9992 - val_loss: 0.0998 - val_acc: 0.9991\n",
            "Epoch 206/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2952 - acc: 0.9993 - val_loss: 0.0994 - val_acc: 0.9991\n",
            "Epoch 207/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2950 - acc: 0.9993 - val_loss: 0.0994 - val_acc: 0.9991\n",
            "Epoch 208/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2948 - acc: 0.9994 - val_loss: 0.0991 - val_acc: 0.9991\n",
            "Epoch 209/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2947 - acc: 0.9994 - val_loss: 0.0988 - val_acc: 0.9992\n",
            "Epoch 210/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2947 - acc: 0.9994 - val_loss: 0.0986 - val_acc: 0.9992\n",
            "Epoch 211/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2946 - acc: 0.9994 - val_loss: 0.0984 - val_acc: 0.9992\n",
            "Epoch 212/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2946 - acc: 0.9994 - val_loss: 0.0982 - val_acc: 0.9992\n",
            "Epoch 213/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2946 - acc: 0.9994 - val_loss: 0.0982 - val_acc: 0.9992\n",
            "Epoch 214/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2945 - acc: 0.9994 - val_loss: 0.0980 - val_acc: 0.9992\n",
            "Epoch 215/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2944 - acc: 0.9995 - val_loss: 0.0978 - val_acc: 0.9992\n",
            "Epoch 216/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2943 - acc: 0.9995 - val_loss: 0.0976 - val_acc: 0.9992\n",
            "Epoch 217/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2943 - acc: 0.9995 - val_loss: 0.0975 - val_acc: 0.9992\n",
            "Epoch 218/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2943 - acc: 0.9995 - val_loss: 0.0973 - val_acc: 0.9992\n",
            "Epoch 219/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2942 - acc: 0.9995 - val_loss: 0.0971 - val_acc: 0.9992\n",
            "Epoch 220/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2942 - acc: 0.9995 - val_loss: 0.0971 - val_acc: 0.9992\n",
            "Epoch 221/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2941 - acc: 0.9995 - val_loss: 0.0970 - val_acc: 0.9992\n",
            "Epoch 222/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2941 - acc: 0.9995 - val_loss: 0.0967 - val_acc: 0.9992\n",
            "Epoch 223/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2941 - acc: 0.9995 - val_loss: 0.0966 - val_acc: 0.9992\n",
            "Epoch 224/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2941 - acc: 0.9995 - val_loss: 0.0965 - val_acc: 0.9992\n",
            "Epoch 225/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2940 - acc: 0.9995 - val_loss: 0.0963 - val_acc: 0.9992\n",
            "Epoch 226/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2940 - acc: 0.9995 - val_loss: 0.0962 - val_acc: 0.9992\n",
            "Epoch 227/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2940 - acc: 0.9995 - val_loss: 0.0962 - val_acc: 0.9992\n",
            "Epoch 228/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2940 - acc: 0.9995 - val_loss: 0.0959 - val_acc: 0.9992\n",
            "Epoch 229/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2940 - acc: 0.9995 - val_loss: 0.0958 - val_acc: 0.9992\n",
            "Epoch 230/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2939 - acc: 0.9995 - val_loss: 0.0957 - val_acc: 0.9992\n",
            "Epoch 231/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2939 - acc: 0.9995 - val_loss: 0.0955 - val_acc: 0.9992\n",
            "Epoch 232/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2939 - acc: 0.9995 - val_loss: 0.0954 - val_acc: 0.9992\n",
            "Epoch 233/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2939 - acc: 0.9995 - val_loss: 0.0954 - val_acc: 0.9992\n",
            "Epoch 234/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2939 - acc: 0.9995 - val_loss: 0.0953 - val_acc: 0.9992\n",
            "Epoch 235/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2938 - acc: 0.9995 - val_loss: 0.0951 - val_acc: 0.9992\n",
            "Epoch 236/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2938 - acc: 0.9995 - val_loss: 0.0951 - val_acc: 0.9992\n",
            "Epoch 237/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2938 - acc: 0.9995 - val_loss: 0.0950 - val_acc: 0.9992\n",
            "Epoch 238/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2937 - acc: 0.9995 - val_loss: 0.0945 - val_acc: 0.9992\n",
            "Epoch 239/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2937 - acc: 0.9995 - val_loss: 0.0944 - val_acc: 0.9992\n",
            "Epoch 240/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2938 - acc: 0.9995 - val_loss: 0.0954 - val_acc: 0.9991\n",
            "Epoch 241/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2960 - acc: 0.9992 - val_loss: 0.0942 - val_acc: 0.9993\n",
            "Epoch 242/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2947 - acc: 0.9993 - val_loss: 0.0945 - val_acc: 0.9992\n",
            "Epoch 243/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2942 - acc: 0.9994 - val_loss: 0.0950 - val_acc: 0.9991\n",
            "Epoch 244/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2939 - acc: 0.9995 - val_loss: 0.0944 - val_acc: 0.9991\n",
            "Epoch 245/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2938 - acc: 0.9995 - val_loss: 0.0942 - val_acc: 0.9991\n",
            "Epoch 246/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2937 - acc: 0.9995 - val_loss: 0.0941 - val_acc: 0.9991\n",
            "Epoch 247/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2937 - acc: 0.9995 - val_loss: 0.0937 - val_acc: 0.9992\n",
            "Epoch 248/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2937 - acc: 0.9995 - val_loss: 0.0936 - val_acc: 0.9992\n",
            "Epoch 249/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2936 - acc: 0.9995 - val_loss: 0.0938 - val_acc: 0.9992\n",
            "Epoch 250/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2936 - acc: 0.9995 - val_loss: 0.0934 - val_acc: 0.9992\n",
            "Epoch 251/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2936 - acc: 0.9995 - val_loss: 0.0932 - val_acc: 0.9993\n",
            "Epoch 252/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2936 - acc: 0.9995 - val_loss: 0.0936 - val_acc: 0.9992\n",
            "Epoch 253/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2935 - acc: 0.9995 - val_loss: 0.0931 - val_acc: 0.9992\n",
            "Epoch 254/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2935 - acc: 0.9995 - val_loss: 0.0930 - val_acc: 0.9992\n",
            "Epoch 255/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2935 - acc: 0.9995 - val_loss: 0.0928 - val_acc: 0.9993\n",
            "Epoch 256/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2935 - acc: 0.9995 - val_loss: 0.0930 - val_acc: 0.9992\n",
            "Epoch 257/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2935 - acc: 0.9995 - val_loss: 0.0924 - val_acc: 0.9993\n",
            "Epoch 258/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2934 - acc: 0.9995 - val_loss: 0.0929 - val_acc: 0.9992\n",
            "Epoch 259/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2935 - acc: 0.9995 - val_loss: 0.0927 - val_acc: 0.9992\n",
            "Epoch 260/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2934 - acc: 0.9995 - val_loss: 0.0925 - val_acc: 0.9992\n",
            "Epoch 261/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2934 - acc: 0.9995 - val_loss: 0.0922 - val_acc: 0.9993\n",
            "Epoch 262/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2934 - acc: 0.9995 - val_loss: 0.0922 - val_acc: 0.9992\n",
            "Epoch 263/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2934 - acc: 0.9995 - val_loss: 0.0920 - val_acc: 0.9993\n",
            "Epoch 264/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2934 - acc: 0.9995 - val_loss: 0.0921 - val_acc: 0.9992\n",
            "Epoch 265/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9995 - val_loss: 0.0919 - val_acc: 0.9992\n",
            "Epoch 266/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9995 - val_loss: 0.0917 - val_acc: 0.9993\n",
            "Epoch 267/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2933 - acc: 0.9995 - val_loss: 0.0916 - val_acc: 0.9993\n",
            "Epoch 268/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9995 - val_loss: 0.0917 - val_acc: 0.9992\n",
            "Epoch 269/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9995 - val_loss: 0.0915 - val_acc: 0.9993\n",
            "Epoch 270/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9995 - val_loss: 0.0912 - val_acc: 0.9993\n",
            "Epoch 271/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2933 - acc: 0.9995 - val_loss: 0.0914 - val_acc: 0.9993\n",
            "Epoch 272/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9995 - val_loss: 0.0913 - val_acc: 0.9993\n",
            "Epoch 273/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9995 - val_loss: 0.0912 - val_acc: 0.9993\n",
            "Epoch 274/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9995 - val_loss: 0.0912 - val_acc: 0.9993\n",
            "Epoch 275/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9995 - val_loss: 0.0911 - val_acc: 0.9993\n",
            "Epoch 276/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9995 - val_loss: 0.0909 - val_acc: 0.9993\n",
            "Epoch 277/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9995 - val_loss: 0.0908 - val_acc: 0.9993\n",
            "Epoch 278/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9995 - val_loss: 0.0909 - val_acc: 0.9993\n",
            "Epoch 279/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9995 - val_loss: 0.0906 - val_acc: 0.9993\n",
            "Epoch 280/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9995 - val_loss: 0.0908 - val_acc: 0.9992\n",
            "Epoch 281/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9995 - val_loss: 0.0906 - val_acc: 0.9993\n",
            "Epoch 282/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9995 - val_loss: 0.0909 - val_acc: 0.9992\n",
            "Epoch 283/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9995 - val_loss: 0.0907 - val_acc: 0.9993\n",
            "Epoch 284/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2934 - acc: 0.9995 - val_loss: 0.0912 - val_acc: 0.9992\n",
            "Epoch 285/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9995 - val_loss: 0.0907 - val_acc: 0.9993\n",
            "Epoch 286/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9995 - val_loss: 0.0905 - val_acc: 0.9993\n",
            "Epoch 287/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0903 - val_acc: 0.9993\n",
            "Epoch 288/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0903 - val_acc: 0.9993\n",
            "Epoch 289/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0901 - val_acc: 0.9993\n",
            "Epoch 290/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0902 - val_acc: 0.9993\n",
            "Epoch 291/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0900 - val_acc: 0.9993\n",
            "Epoch 292/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0901 - val_acc: 0.9993\n",
            "Epoch 293/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0898 - val_acc: 0.9993\n",
            "Epoch 294/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0900 - val_acc: 0.9993\n",
            "Epoch 295/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0897 - val_acc: 0.9993\n",
            "Epoch 296/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0896 - val_acc: 0.9993\n",
            "Epoch 297/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0899 - val_acc: 0.9993\n",
            "Epoch 298/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0897 - val_acc: 0.9993\n",
            "Epoch 299/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0896 - val_acc: 0.9993\n",
            "Epoch 300/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0894 - val_acc: 0.9993\n",
            "Epoch 301/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0893 - val_acc: 0.9993\n",
            "Epoch 302/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0893 - val_acc: 0.9993\n",
            "Epoch 303/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0893 - val_acc: 0.9993\n",
            "Epoch 304/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9996 - val_loss: 0.0894 - val_acc: 0.9993\n",
            "Epoch 305/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0893 - val_acc: 0.9993\n",
            "Epoch 306/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0894 - val_acc: 0.9993\n",
            "Epoch 307/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0892 - val_acc: 0.9993\n",
            "Epoch 308/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9996 - val_loss: 0.0894 - val_acc: 0.9993\n",
            "Epoch 309/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0890 - val_acc: 0.9993\n",
            "Epoch 310/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0894 - val_acc: 0.9993\n",
            "Epoch 311/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0888 - val_acc: 0.9993\n",
            "Epoch 312/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9996 - val_loss: 0.0889 - val_acc: 0.9993\n",
            "Epoch 313/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9995 - val_loss: 0.0886 - val_acc: 0.9994\n",
            "Epoch 314/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9996 - val_loss: 0.0886 - val_acc: 0.9993\n",
            "Epoch 315/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9996 - val_loss: 0.0887 - val_acc: 0.9993\n",
            "Epoch 316/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9996 - val_loss: 0.0887 - val_acc: 0.9993\n",
            "Epoch 317/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9996 - val_loss: 0.0885 - val_acc: 0.9993\n",
            "Epoch 318/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0884 - val_acc: 0.9993\n",
            "Epoch 319/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0884 - val_acc: 0.9993\n",
            "Epoch 320/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0884 - val_acc: 0.9993\n",
            "Epoch 321/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0887 - val_acc: 0.9993\n",
            "Epoch 322/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9996 - val_loss: 0.0884 - val_acc: 0.9993\n",
            "Epoch 323/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9996 - val_loss: 0.0881 - val_acc: 0.9994\n",
            "Epoch 324/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9996 - val_loss: 0.0884 - val_acc: 0.9993\n",
            "Epoch 325/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0883 - val_acc: 0.9993\n",
            "Epoch 326/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0881 - val_acc: 0.9993\n",
            "Epoch 327/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0881 - val_acc: 0.9993\n",
            "Epoch 328/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0881 - val_acc: 0.9993\n",
            "Epoch 329/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0879 - val_acc: 0.9994\n",
            "Epoch 330/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0881 - val_acc: 0.9993\n",
            "Epoch 331/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0881 - val_acc: 0.9993\n",
            "Epoch 332/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0879 - val_acc: 0.9993\n",
            "Epoch 333/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0878 - val_acc: 0.9994\n",
            "Epoch 334/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0878 - val_acc: 0.9993\n",
            "Epoch 335/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0877 - val_acc: 0.9994\n",
            "Epoch 336/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0878 - val_acc: 0.9993\n",
            "Epoch 337/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0878 - val_acc: 0.9993\n",
            "Epoch 338/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0876 - val_acc: 0.9994\n",
            "Epoch 339/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0877 - val_acc: 0.9993\n",
            "Epoch 340/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0876 - val_acc: 0.9993\n",
            "Epoch 341/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0876 - val_acc: 0.9993\n",
            "Epoch 342/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0876 - val_acc: 0.9993\n",
            "Epoch 343/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0875 - val_acc: 0.9993\n",
            "Epoch 344/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0875 - val_acc: 0.9993\n",
            "Epoch 345/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9996 - val_loss: 0.0875 - val_acc: 0.9993\n",
            "Epoch 346/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0874 - val_acc: 0.9993\n",
            "Epoch 347/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0874 - val_acc: 0.9993\n",
            "Epoch 348/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0873 - val_acc: 0.9993\n",
            "Epoch 349/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0873 - val_acc: 0.9993\n",
            "Epoch 350/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0872 - val_acc: 0.9993\n",
            "Epoch 351/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0872 - val_acc: 0.9993\n",
            "Epoch 352/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0873 - val_acc: 0.9993\n",
            "Epoch 353/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0871 - val_acc: 0.9993\n",
            "Epoch 354/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0871 - val_acc: 0.9993\n",
            "Epoch 355/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0871 - val_acc: 0.9993\n",
            "Epoch 356/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0870 - val_acc: 0.9993\n",
            "Epoch 357/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0870 - val_acc: 0.9993\n",
            "Epoch 358/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0870 - val_acc: 0.9993\n",
            "Epoch 359/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0869 - val_acc: 0.9993\n",
            "Epoch 360/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0870 - val_acc: 0.9993\n",
            "Epoch 361/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0869 - val_acc: 0.9993\n",
            "Epoch 362/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0869 - val_acc: 0.9993\n",
            "Epoch 363/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0868 - val_acc: 0.9993\n",
            "Epoch 364/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0868 - val_acc: 0.9993\n",
            "Epoch 365/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0869 - val_acc: 0.9993\n",
            "Epoch 366/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0868 - val_acc: 0.9994\n",
            "Epoch 367/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0867 - val_acc: 0.9993\n",
            "Epoch 368/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0867 - val_acc: 0.9993\n",
            "Epoch 369/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0868 - val_acc: 0.9993\n",
            "Epoch 370/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0867 - val_acc: 0.9993\n",
            "Epoch 371/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0867 - val_acc: 0.9993\n",
            "Epoch 372/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0866 - val_acc: 0.9993\n",
            "Epoch 373/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0866 - val_acc: 0.9993\n",
            "Epoch 374/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0866 - val_acc: 0.9993\n",
            "Epoch 375/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0865 - val_acc: 0.9993\n",
            "Epoch 376/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0865 - val_acc: 0.9993\n",
            "Epoch 377/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0865 - val_acc: 0.9993\n",
            "Epoch 378/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0864 - val_acc: 0.9993\n",
            "Epoch 379/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0865 - val_acc: 0.9993\n",
            "Epoch 380/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0864 - val_acc: 0.9993\n",
            "Epoch 381/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0864 - val_acc: 0.9993\n",
            "Epoch 382/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0863 - val_acc: 0.9993\n",
            "Epoch 383/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0863 - val_acc: 0.9993\n",
            "Epoch 384/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0864 - val_acc: 0.9993\n",
            "Epoch 385/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0863 - val_acc: 0.9993\n",
            "Epoch 386/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0863 - val_acc: 0.9993\n",
            "Epoch 387/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0862 - val_acc: 0.9993\n",
            "Epoch 388/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0864 - val_acc: 0.9993\n",
            "Epoch 389/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0863 - val_acc: 0.9993\n",
            "Epoch 390/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0862 - val_acc: 0.9993\n",
            "Epoch 391/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0863 - val_acc: 0.9993\n",
            "Epoch 392/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0861 - val_acc: 0.9994\n",
            "Epoch 393/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0861 - val_acc: 0.9993\n",
            "Epoch 394/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0861 - val_acc: 0.9993\n",
            "Epoch 395/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0862 - val_acc: 0.9993\n",
            "Epoch 396/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0863 - val_acc: 0.9993\n",
            "Epoch 397/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0860 - val_acc: 0.9993\n",
            "Epoch 398/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0861 - val_acc: 0.9993\n",
            "Epoch 399/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0861 - val_acc: 0.9993\n",
            "Epoch 400/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0861 - val_acc: 0.9993\n",
            "Epoch 401/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0861 - val_acc: 0.9993\n",
            "Epoch 402/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0860 - val_acc: 0.9993\n",
            "Epoch 403/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0859 - val_acc: 0.9993\n",
            "Epoch 404/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0859 - val_acc: 0.9993\n",
            "Epoch 405/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0860 - val_acc: 0.9993\n",
            "Epoch 406/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0859 - val_acc: 0.9993\n",
            "Epoch 407/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0859 - val_acc: 0.9993\n",
            "Epoch 408/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0860 - val_acc: 0.9993\n",
            "Epoch 409/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0860 - val_acc: 0.9993\n",
            "Epoch 410/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0859 - val_acc: 0.9993\n",
            "Epoch 411/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0862 - val_acc: 0.9993\n",
            "Epoch 412/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0858 - val_acc: 0.9993\n",
            "Epoch 413/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0859 - val_acc: 0.9993\n",
            "Epoch 414/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0860 - val_acc: 0.9993\n",
            "Epoch 415/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0859 - val_acc: 0.9993\n",
            "Epoch 416/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0857 - val_acc: 0.9993\n",
            "Epoch 417/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0859 - val_acc: 0.9993\n",
            "Epoch 418/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0859 - val_acc: 0.9993\n",
            "Epoch 419/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0858 - val_acc: 0.9993\n",
            "Epoch 420/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0857 - val_acc: 0.9993\n",
            "Epoch 421/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0857 - val_acc: 0.9993\n",
            "Epoch 422/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0859 - val_acc: 0.9993\n",
            "Epoch 423/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2935 - acc: 0.9995 - val_loss: 0.0864 - val_acc: 0.9993\n",
            "Epoch 424/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3078 - acc: 0.9979 - val_loss: 0.0951 - val_acc: 0.9978\n",
            "Epoch 425/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3098 - acc: 0.9975 - val_loss: 0.0891 - val_acc: 0.9983\n",
            "Epoch 426/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2978 - acc: 0.9986 - val_loss: 0.0885 - val_acc: 0.9985\n",
            "Epoch 427/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2960 - acc: 0.9988 - val_loss: 0.0879 - val_acc: 0.9986\n",
            "Epoch 428/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2949 - acc: 0.9990 - val_loss: 0.0869 - val_acc: 0.9989\n",
            "Epoch 429/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2945 - acc: 0.9991 - val_loss: 0.0870 - val_acc: 0.9989\n",
            "Epoch 430/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2942 - acc: 0.9991 - val_loss: 0.0868 - val_acc: 0.9990\n",
            "Epoch 431/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2939 - acc: 0.9992 - val_loss: 0.0867 - val_acc: 0.9991\n",
            "Epoch 432/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2938 - acc: 0.9992 - val_loss: 0.0864 - val_acc: 0.9991\n",
            "Epoch 433/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2936 - acc: 0.9993 - val_loss: 0.0864 - val_acc: 0.9991\n",
            "Epoch 434/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2935 - acc: 0.9994 - val_loss: 0.0864 - val_acc: 0.9991\n",
            "Epoch 435/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9994 - val_loss: 0.0862 - val_acc: 0.9991\n",
            "Epoch 436/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9994 - val_loss: 0.0862 - val_acc: 0.9991\n",
            "Epoch 437/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3081 - acc: 0.9974 - val_loss: 0.0918 - val_acc: 0.9981\n",
            "Epoch 438/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.3008 - acc: 0.9984 - val_loss: 0.0920 - val_acc: 0.9981\n",
            "Epoch 439/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2976 - acc: 0.9987 - val_loss: 0.0879 - val_acc: 0.9986\n",
            "Epoch 440/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2957 - acc: 0.9989 - val_loss: 0.0875 - val_acc: 0.9987\n",
            "Epoch 441/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2949 - acc: 0.9990 - val_loss: 0.0868 - val_acc: 0.9989\n",
            "Epoch 442/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2945 - acc: 0.9991 - val_loss: 0.0865 - val_acc: 0.9990\n",
            "Epoch 443/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2942 - acc: 0.9991 - val_loss: 0.0862 - val_acc: 0.9990\n",
            "Epoch 444/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2940 - acc: 0.9992 - val_loss: 0.0862 - val_acc: 0.9990\n",
            "Epoch 445/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2937 - acc: 0.9992 - val_loss: 0.0861 - val_acc: 0.9990\n",
            "Epoch 446/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2936 - acc: 0.9993 - val_loss: 0.0862 - val_acc: 0.9990\n",
            "Epoch 447/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2935 - acc: 0.9993 - val_loss: 0.0860 - val_acc: 0.9991\n",
            "Epoch 448/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2934 - acc: 0.9993 - val_loss: 0.0860 - val_acc: 0.9991\n",
            "Epoch 449/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9994 - val_loss: 0.0858 - val_acc: 0.9991\n",
            "Epoch 450/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2933 - acc: 0.9994 - val_loss: 0.0859 - val_acc: 0.9991\n",
            "Epoch 451/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9994 - val_loss: 0.0857 - val_acc: 0.9991\n",
            "Epoch 452/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2932 - acc: 0.9994 - val_loss: 0.0858 - val_acc: 0.9991\n",
            "Epoch 453/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9994 - val_loss: 0.0857 - val_acc: 0.9991\n",
            "Epoch 454/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0857 - val_acc: 0.9991\n",
            "Epoch 455/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0858 - val_acc: 0.9991\n",
            "Epoch 456/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0856 - val_acc: 0.9992\n",
            "Epoch 457/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2931 - acc: 0.9995 - val_loss: 0.0856 - val_acc: 0.9992\n",
            "Epoch 458/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0856 - val_acc: 0.9992\n",
            "Epoch 459/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0857 - val_acc: 0.9992\n",
            "Epoch 460/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0857 - val_acc: 0.9992\n",
            "Epoch 461/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0856 - val_acc: 0.9992\n",
            "Epoch 462/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2930 - acc: 0.9995 - val_loss: 0.0854 - val_acc: 0.9992\n",
            "Epoch 463/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9995 - val_loss: 0.0855 - val_acc: 0.9992\n",
            "Epoch 464/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9995 - val_loss: 0.0855 - val_acc: 0.9992\n",
            "Epoch 465/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9995 - val_loss: 0.0856 - val_acc: 0.9992\n",
            "Epoch 466/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9995 - val_loss: 0.0855 - val_acc: 0.9992\n",
            "Epoch 467/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9995 - val_loss: 0.0856 - val_acc: 0.9992\n",
            "Epoch 468/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9995 - val_loss: 0.0854 - val_acc: 0.9992\n",
            "Epoch 469/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2929 - acc: 0.9995 - val_loss: 0.0854 - val_acc: 0.9992\n",
            "Epoch 470/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9995 - val_loss: 0.0853 - val_acc: 0.9992\n",
            "Epoch 471/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2929 - acc: 0.9995 - val_loss: 0.0854 - val_acc: 0.9992\n",
            "Epoch 472/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9995 - val_loss: 0.0853 - val_acc: 0.9992\n",
            "Epoch 473/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9995 - val_loss: 0.0852 - val_acc: 0.9993\n",
            "Epoch 474/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9995 - val_loss: 0.0853 - val_acc: 0.9993\n",
            "Epoch 475/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9995 - val_loss: 0.0853 - val_acc: 0.9992\n",
            "Epoch 476/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9995 - val_loss: 0.0854 - val_acc: 0.9992\n",
            "Epoch 477/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9995 - val_loss: 0.0854 - val_acc: 0.9992\n",
            "Epoch 478/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9995 - val_loss: 0.0853 - val_acc: 0.9993\n",
            "Epoch 479/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2928 - acc: 0.9995 - val_loss: 0.0852 - val_acc: 0.9993\n",
            "Epoch 480/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9995 - val_loss: 0.0853 - val_acc: 0.9992\n",
            "Epoch 481/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9995 - val_loss: 0.0853 - val_acc: 0.9993\n",
            "Epoch 482/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0853 - val_acc: 0.9992\n",
            "Epoch 483/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0852 - val_acc: 0.9993\n",
            "Epoch 484/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0853 - val_acc: 0.9993\n",
            "Epoch 485/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0852 - val_acc: 0.9993\n",
            "Epoch 486/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0852 - val_acc: 0.9993\n",
            "Epoch 487/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0852 - val_acc: 0.9993\n",
            "Epoch 488/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0852 - val_acc: 0.9993\n",
            "Epoch 489/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0852 - val_acc: 0.9993\n",
            "Epoch 490/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0851 - val_acc: 0.9993\n",
            "Epoch 491/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0852 - val_acc: 0.9993\n",
            "Epoch 492/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0851 - val_acc: 0.9993\n",
            "Epoch 493/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0850 - val_acc: 0.9993\n",
            "Epoch 494/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0851 - val_acc: 0.9993\n",
            "Epoch 495/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0851 - val_acc: 0.9993\n",
            "Epoch 496/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0851 - val_acc: 0.9993\n",
            "Epoch 497/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0850 - val_acc: 0.9993\n",
            "Epoch 498/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0851 - val_acc: 0.9993\n",
            "Epoch 499/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0850 - val_acc: 0.9993\n",
            "Epoch 500/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0850 - val_acc: 0.9993\n",
            "Epoch 501/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0850 - val_acc: 0.9993\n",
            "Epoch 502/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0850 - val_acc: 0.9993\n",
            "Epoch 503/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0850 - val_acc: 0.9994\n",
            "Epoch 504/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0850 - val_acc: 0.9993\n",
            "Epoch 505/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0850 - val_acc: 0.9993\n",
            "Epoch 506/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9994\n",
            "Epoch 507/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2927 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9993\n",
            "Epoch 508/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0850 - val_acc: 0.9993\n",
            "Epoch 509/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9994\n",
            "Epoch 510/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9993\n",
            "Epoch 511/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9993\n",
            "Epoch 512/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 513/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9993\n",
            "Epoch 514/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9993\n",
            "Epoch 515/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9993\n",
            "Epoch 516/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9993\n",
            "Epoch 517/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 518/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9994\n",
            "Epoch 519/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9993\n",
            "Epoch 520/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 521/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9993\n",
            "Epoch 522/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9993\n",
            "Epoch 523/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 524/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9994\n",
            "Epoch 525/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9994\n",
            "Epoch 526/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 527/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0849 - val_acc: 0.9993\n",
            "Epoch 528/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9994\n",
            "Epoch 529/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 530/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 531/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 532/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 533/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 534/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 535/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9994\n",
            "Epoch 536/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9994\n",
            "Epoch 537/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9994\n",
            "Epoch 538/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9994\n",
            "Epoch 539/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 540/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 541/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 542/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 543/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 544/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9994\n",
            "Epoch 545/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 546/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9994\n",
            "Epoch 547/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 548/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 549/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 550/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 551/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 552/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 553/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 554/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 555/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 556/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 557/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 558/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 559/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 560/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 561/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 562/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 563/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 564/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 565/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 566/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9994\n",
            "Epoch 567/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 568/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 569/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 570/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9994\n",
            "Epoch 571/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9994\n",
            "Epoch 572/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9994\n",
            "Epoch 573/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 574/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 575/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 576/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9994\n",
            "Epoch 577/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 578/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 579/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9994\n",
            "Epoch 580/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 581/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2926 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9994\n",
            "Epoch 582/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 583/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 584/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 585/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 586/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 587/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 588/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 589/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 590/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 591/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 592/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 593/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 594/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 595/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 596/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 597/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 598/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 599/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 600/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 601/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 602/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 603/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 604/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 605/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 606/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 607/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 608/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 609/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 610/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 611/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 612/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 613/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 614/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 615/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 616/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 617/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 618/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 619/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 620/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 621/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 622/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 623/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 624/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 625/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 626/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 627/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 628/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 629/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 630/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 631/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 632/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 633/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 634/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 635/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 636/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 637/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 638/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 639/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 640/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 641/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 642/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 643/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 644/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 645/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 646/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 647/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 648/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 649/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 650/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 651/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 652/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 653/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 654/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 655/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 656/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 657/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 658/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 659/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 660/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 661/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 662/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 663/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 664/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 665/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 666/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0845 - val_acc: 0.9993\n",
            "Epoch 667/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 668/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 669/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 670/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 671/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 672/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 673/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 674/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 675/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 676/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 677/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 678/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 679/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 680/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 681/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 682/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 683/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 684/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 685/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 686/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 687/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 688/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 689/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 690/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 691/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 692/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 693/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 694/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 695/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 696/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 697/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 698/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 699/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 700/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 701/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 702/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 703/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 704/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 705/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 706/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 707/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 708/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 709/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 710/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 711/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 712/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 713/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 714/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 715/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 716/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 717/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 718/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 719/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 720/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 721/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 722/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 723/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 724/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 725/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 726/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 727/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 728/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 729/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 730/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 731/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 732/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 733/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 734/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 735/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 736/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 737/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 738/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 739/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 740/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 741/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 742/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 743/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 744/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 745/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 746/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 747/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 748/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 749/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 750/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 751/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 752/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 753/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 754/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 755/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 756/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 757/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 758/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 759/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 760/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 761/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 762/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 763/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 764/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 765/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 766/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 767/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 768/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 769/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 770/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 771/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 772/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 773/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 774/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 775/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 776/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 777/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 778/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 779/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 780/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 781/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 782/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 783/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 784/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 785/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 786/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 787/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 788/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 789/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 790/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 791/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 792/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 793/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 794/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 795/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 796/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 797/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 798/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 799/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 800/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 801/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 802/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 803/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 804/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 805/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 806/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 807/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 808/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 809/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 810/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 811/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 812/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 813/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 814/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 815/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 816/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 817/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 818/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 819/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 820/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 821/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 822/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 823/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 824/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 825/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 826/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 827/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 828/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 829/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 830/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 831/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 832/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 833/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 834/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0848 - val_acc: 0.9993\n",
            "Epoch 835/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 836/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 837/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 838/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 839/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 840/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 841/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 842/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 843/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 844/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 845/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 846/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 847/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 848/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 849/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 850/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 851/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 852/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 853/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 854/1000\n",
            "227845/227845 [==============================] - 1s 5us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 855/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 856/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 857/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 858/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 859/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 860/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 861/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 862/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 863/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 864/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 865/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 866/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 867/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 868/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 869/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 870/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 871/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 872/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 873/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 874/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 875/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 876/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 877/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 878/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 879/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 880/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 881/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 882/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 883/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 884/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 885/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 886/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 887/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 888/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 889/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 890/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 891/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 892/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 893/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 894/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 895/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 896/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 897/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 898/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 899/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 900/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 901/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 902/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 903/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 904/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 905/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 906/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 907/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 908/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 909/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 910/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 911/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 912/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 913/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 914/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 915/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 916/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 917/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 918/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 919/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 920/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 921/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0845 - val_acc: 0.9993\n",
            "Epoch 922/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 923/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 924/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 925/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 926/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 927/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 928/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 929/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 930/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 931/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 932/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 933/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 934/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 935/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 936/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 937/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 938/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 939/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 940/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 941/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 942/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 943/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 944/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 945/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 946/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 947/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 948/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 949/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 950/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 951/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 952/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 953/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 954/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 955/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 956/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 957/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 958/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 959/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 960/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 961/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 962/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 963/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 964/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 965/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 966/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 967/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 968/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 969/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 970/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 971/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 972/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 973/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 974/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 975/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 976/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 977/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 978/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 979/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 980/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 981/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 982/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 983/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 984/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 985/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 986/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 987/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 988/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 989/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 990/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 991/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 992/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 993/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 994/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 995/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 996/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 997/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 998/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n",
            "Epoch 999/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0847 - val_acc: 0.9993\n",
            "Epoch 1000/1000\n",
            "227845/227845 [==============================] - 1s 4us/step - loss: 0.2925 - acc: 0.9996 - val_loss: 0.0846 - val_acc: 0.9993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FEOjW4sNU__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz47STp3OYlF",
        "colab_type": "code",
        "outputId": "50ca7b64-48af-4c41-954f-f84bd31463ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "pyplot.plot(history_2.history['loss'])\n",
        "pyplot.plot(history_2.history['val_loss'])\n",
        "pyplot.title('train vs validation loss')\n",
        "pyplot.ylabel('loss')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'validation'], loc='upper right')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEcCAYAAAAydkhNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6iVBQgghRL0kSAghhKiXBAkhhBD1+n/bgg65HQhRagAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5un5-GybRLP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_predict = model.predict_classes(X_train_pca)\n",
        "test_predict = model.predict_classes(X_test_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I68qQKQojTfC",
        "colab_type": "code",
        "outputId": "0d2aad86-17fb-46c0-914a-2e82b70d0599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "test_predict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0],\n",
              "       [0],\n",
              "       ...,\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuB4LonvjGEB",
        "colab_type": "code",
        "outputId": "30a227e5-ceb3-4286-decd-13933d9e1cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "metrics.accuracy_score(test_predict, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9993153330290369"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CruIb_VlRoGn",
        "colab_type": "code",
        "outputId": "d8fb72ae-c263-4181-adae-3ab6c1dc453f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "conf_matrix = confusion_matrix(y_test, test_predict)\n",
        "print(conf_matrix)\n",
        "\n",
        "fraud_conditional_prob = conf_matrix[1,1] / (conf_matrix[1,0] + conf_matrix[1,1])\n",
        "print(fraud_conditional_prob)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[56840    24]\n",
            " [   15    83]]\n",
            "0.8469387755102041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BpLjrJskjhb",
        "colab_type": "text"
      },
      "source": [
        "Compared to the result from RandomForest, this model has a lower accuracy but a better performance in terms of identifying fraud. In other words, The model classifies more individual data as fraud, and it identifies more frauds than Random Forest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paX8NdnkbhOt",
        "colab_type": "text"
      },
      "source": [
        "The reason is probably that we did not resample the data for training the randomforest, so it was overfitted on class 0. As a result, it predicts more data as class 0 and reults in higher accuracy. However, the actual performance is worse than the RNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rgy9HXvlvfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}